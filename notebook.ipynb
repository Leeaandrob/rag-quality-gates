{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Naive RAG with RAGAS Evaluation\n\n## Educational Notebook - Bitcoin Whitepaper\n\nThis notebook demonstrates:\n1. **Document Ingestion**: Load and chunk PDF documents\n2. **Vector Store**: Create embeddings with local Nomic model via TEI\n3. **RAG Pipeline**: Query using Claudex (Claude) or Ollama\n4. **RAGAS Evaluation**: Measure quality metrics with Quality Gates\n\n### Infrastructure (100% Local)\n- **TEI Server**: `http://localhost:8080` - Nomic embeddings\n- **Claudex**: `http://localhost:8081` - Claude CLI wrapper ([GitHub](https://github.com/Leeaandrob/claudex))\n- **Ollama**: Fallback with qwen2.5:3b\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration - 100% Local Infrastructure\nimport requests\n\n# Local infrastructure endpoints\nTEI_URL = \"http://localhost:8080\"\nCLAUDEX_URL = \"http://localhost:8081/v1\"\nOLLAMA_MODEL = \"qwen2.5:3b\"\n\n# Choose LLM backend\nUSE_CLAUDEX = True  # Set to False to use Ollama instead\n\n# Verify TEI is running\ntry:\n    response = requests.get(f\"{TEI_URL}/health\", timeout=5)\n    print(f\"‚úÖ TEI Server: {TEI_URL} - Healthy\")\nexcept:\n    print(f\"‚ùå TEI Server not available at {TEI_URL}\")\n    print(\"   Run: docker run -p 8080:80 ghcr.io/huggingface/text-embeddings-inference:cpu-1.5 --model-id nomic-ai/nomic-embed-text-v1.5\")\n\n# Verify Claudex or Ollama\nif USE_CLAUDEX:\n    try:\n        response = requests.get(f\"{CLAUDEX_URL.replace('/v1', '')}/health\", timeout=5)\n        print(f\"‚úÖ Claudex Server: {CLAUDEX_URL} - Healthy\")\n    except:\n        print(f\"‚ö†Ô∏è Claudex not available, will fall back to Ollama\")\n        USE_CLAUDEX = False\nelse:\n    print(f\"‚ÑπÔ∏è Using Ollama with model: {OLLAMA_MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Document Ingestion\n",
    "\n",
    "We'll load the Bitcoin whitepaper and split it into chunks.\n",
    "\n",
    "**Key parameters:**\n",
    "- `chunk_size`: Target size for each chunk (in characters)\n",
    "- `chunk_overlap`: Overlap between consecutive chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document_loader import DocumentProcessor, analyze_chunks\n",
    "\n",
    "# Initialize processor\n",
    "processor = DocumentProcessor(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# Load and chunk the PDF\n",
    "chunks = processor.process(\"bitcoin_paper.pdf\")\n",
    "\n",
    "# Analyze the chunks\n",
    "stats = analyze_chunks(chunks)\n",
    "print(\"\\nChunk Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a sample chunk\n",
    "print(\"Sample Chunk (index 0):\")\n",
    "print(\"-\" * 40)\n",
    "print(chunks[0].page_content)\n",
    "print(\"-\" * 40)\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Vector Store Creation\n\nConvert chunks to embeddings using **local Nomic model via TEI** and store in FAISS.\n\n**How it works:**\n1. Each chunk is sent to TEI server for embedding (Nomic model)\n2. FAISS indexes these vectors for efficient similarity search\n3. Queries are also embedded and compared to find similar chunks\n\n**Local Setup**: No API keys needed - all processing happens locally!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.vector_store import VectorStoreManager\n\n# Initialize vector store with local TEI embeddings\nvector_manager = VectorStoreManager(\n    use_local=True,\n    tei_url=TEI_URL,\n)\n\n# Create index from chunks\nvector_manager.create_from_documents(chunks)\n\nprint(f\"‚úÖ Vector store created with {len(chunks)} chunks using Nomic embeddings\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval with similarity scores\n",
    "query = \"What is proof of work?\"\n",
    "\n",
    "results = vector_manager.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\nTop 3 Results:\")\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\n[{i}] Score: {score:.4f} | Page: {doc.metadata.get('page', '?')}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: RAG Pipeline\n\nCombine retrieval with LLM generation using **Claudex** (Claude) or **Ollama**.\n\n**Pipeline:**\n1. User asks a question\n2. Retrieve k most similar chunks from FAISS\n3. Construct prompt with question + context\n4. Generate answer using LLM (Claudex or Ollama)\n\n**LLM Options:**\n- **Claudex**: Claude via OpenAI-compatible API (recommended)\n- **Ollama**: Local qwen2.5:3b (fallback)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.rag_pipeline import NaiveRAG\n\n# Initialize RAG pipeline with local LLM\nrag = NaiveRAG(\n    vector_store_manager=vector_manager,\n    use_local_llm=not USE_CLAUDEX,\n    use_claudex=USE_CLAUDEX,\n    claudex_url=CLAUDEX_URL,\n    ollama_model=OLLAMA_MODEL,\n    temperature=0.0,\n    k=4,\n)\n\nif USE_CLAUDEX:\n    print(f\"‚úÖ RAG initialized with Claudex at {CLAUDEX_URL}\")\nelse:\n    print(f\"‚úÖ RAG initialized with Ollama ({OLLAMA_MODEL})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a question\n",
    "question = \"What is Bitcoin and how does it work?\"\n",
    "\n",
    "result = rag.query(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nResponse:\")\n",
    "print(result[\"response\"])\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"\\nRetrieved {len(result['retrieved_contexts'])} context chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View retrieved contexts\n",
    "print(\"Retrieved Contexts:\")\n",
    "for i, ctx in enumerate(result[\"retrieved_contexts\"], 1):\n",
    "    print(f\"\\n[Context {i}]\")\n",
    "    print(ctx[:300] + \"...\" if len(ctx) > 300 else ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: RAGAS Evaluation with Quality Gates\n\nEvaluate the RAG pipeline using RAGAS metrics:\n\n| Metric | What It Measures | Quality Gate |\n|--------|------------------|--------------|\n| **Faithfulness** | Is the answer grounded in context? | ‚â• 0.7 |\n| **Answer Relevancy** | Is the answer relevant to the question? | ‚â• 0.8 |\n\n**Why Quality Gates Matter:**\n- Without metrics, you don't know what you're putting in production\n- Small models (qwen2.5:3b) scored 0.691 average\n- Large models (Claude via Claudex) scored 0.906 average (+31% improvement)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluator import RAGASEvaluator, create_test_questions_bitcoin\n",
    "\n",
    "# Create test questions\n",
    "questions, references = create_test_questions_bitcoin()\n",
    "\n",
    "print(f\"Created {len(questions)} test questions:\")\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"  {i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all questions through RAG\n",
    "results = rag.batch_query(questions, references)\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize evaluator with local LLM (same as RAG pipeline)\nevaluator = RAGASEvaluator(\n    metrics=[\"faithfulness\", \"answer_relevancy\"],\n    use_local=not USE_CLAUDEX,\n    use_claudex=USE_CLAUDEX,\n    claudex_url=CLAUDEX_URL,\n    ollama_model=OLLAMA_MODEL,\n    tei_url=TEI_URL,\n)\n\n# Run evaluation\nprint(\"Running RAGAS evaluation...\")\nevaluation = evaluator.evaluate(results)\nprint(\"‚úÖ Evaluation complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# View overall scores with Quality Gate validation\nevaluator.print_report(evaluation)\n\n# Quality Gate Check\nFAITHFULNESS_THRESHOLD = 0.7\nRELEVANCY_THRESHOLD = 0.8\n\nscores = evaluation[\"scores\"]\nfaithfulness_pass = scores.get(\"faithfulness\", 0) >= FAITHFULNESS_THRESHOLD\nrelevancy_pass = scores.get(\"answer_relevancy\", 0) >= RELEVANCY_THRESHOLD\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"QUALITY GATE VALIDATION\")\nprint(\"=\" * 50)\nprint(f\"Faithfulness: {scores.get('faithfulness', 0):.3f} {'‚úÖ PASS' if faithfulness_pass else '‚ùå FAIL'} (threshold: {FAITHFULNESS_THRESHOLD})\")\nprint(f\"Relevancy:    {scores.get('answer_relevancy', 0):.3f} {'‚úÖ PASS' if relevancy_pass else '‚ùå FAIL'} (threshold: {RELEVANCY_THRESHOLD})\")\nprint(\"=\" * 50)\nif faithfulness_pass and relevancy_pass:\n    print(\"üéâ ALL QUALITY GATES PASSED - Production Ready!\")\nelse:\n    print(\"‚ö†Ô∏è QUALITY GATES FAILED - Needs Improvement\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detailed results DataFrame\ndf = evaluation[\"dataframe\"]\ndf[[\"user_input\", \"faithfulness\", \"answer_relevancy\"]]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analysis & Experimentation\n",
    "\n",
    "Let's analyze the results and identify areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find questions with lowest scores\nprint(\"Questions with LOWEST faithfulness:\")\nfor _, row in df.nsmallest(3, \"faithfulness\").iterrows():\n    print(f\"  Score: {row['faithfulness']:.2f} | {row['user_input'][:60]}\")\n\nprint(\"\\nQuestions with LOWEST answer_relevancy:\")\nfor _, row in df.nsmallest(3, \"answer_relevancy\").iterrows():\n    print(f\"  Score: {row['answer_relevancy']:.2f} | {row['user_input'][:60]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize scores distribution\nimport matplotlib.pyplot as plt\n\nmetrics = [\"faithfulness\", \"answer_relevancy\"]\nthresholds = {\"faithfulness\": 0.7, \"answer_relevancy\": 0.8}\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nfor i, metric in enumerate(metrics):\n    axes[i].hist(df[metric].dropna(), bins=10, edgecolor='black', alpha=0.7, color='steelblue')\n    axes[i].set_title(f\"{metric.replace('_', ' ').title()}\")\n    axes[i].set_xlabel(\"Score\")\n    axes[i].set_ylabel(\"Count\")\n    axes[i].axvline(df[metric].mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean: {df[metric].mean():.2f}')\n    axes[i].axvline(thresholds[metric], color='red', linestyle='-', linewidth=2, label=f'Threshold: {thresholds[metric]}')\n    axes[i].legend()\n    axes[i].set_xlim(0, 1)\n\nplt.tight_layout()\nplt.suptitle(\"RAGAS Score Distribution with Quality Gate Thresholds\", y=1.02, fontsize=14)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Try different configurations and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Experiment: Different chunk sizes\nchunk_sizes = [300, 500, 800]\nexperiment_results = {}\n\nfor chunk_size in chunk_sizes:\n    print(f\"\\nTesting chunk_size={chunk_size}\")\n    \n    # Create new processor\n    processor = DocumentProcessor(chunk_size=chunk_size, chunk_overlap=100)\n    chunks = processor.process(\"bitcoin_paper.pdf\")\n    \n    # Create new vector store with local embeddings\n    vm = VectorStoreManager(use_local=True, tei_url=TEI_URL)\n    vm.create_from_documents(chunks)\n    \n    # Create new RAG with local LLM\n    test_rag = NaiveRAG(\n        vm, \n        k=4,\n        use_local_llm=not USE_CLAUDEX,\n        use_claudex=USE_CLAUDEX,\n        claudex_url=CLAUDEX_URL,\n        ollama_model=OLLAMA_MODEL,\n    )\n    \n    # Run on sample questions (just 3 for speed)\n    test_results = test_rag.batch_query(questions[:3], references[:3])\n    \n    # Evaluate\n    eval_result = evaluator.evaluate(test_results)\n    experiment_results[chunk_size] = eval_result[\"scores\"]\n\n# Compare results\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Experiment Results:\")\nfor chunk_size, scores in experiment_results.items():\n    print(f\"\\nChunk Size: {chunk_size}\")\n    for metric, score in scores.items():\n        print(f\"  {metric}: {score:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "evaluator.save_results(evaluation, \"outputs/evaluation_results.csv\")\n",
    "\n",
    "# Save vector store for later use\n",
    "vector_manager.save(\"data/faiss_index\")\n",
    "\n",
    "print(\"Results saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}